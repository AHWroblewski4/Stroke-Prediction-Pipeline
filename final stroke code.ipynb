{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Stroke Prediction Pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Import Libaries\n",
        "from pyspark.sql import SparkSession, DataFrame\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import Imputer, StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
        "from pyspark.ml.classification import LogisticRegression, LinearSVC, GBTClassifier, MultilayerPerceptronClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "import time\n",
        "import logging\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Data information\n",
        "label_column = \"stroke\"\n",
        "csv_input_path = \"hdfs://master:9000/user/sat3812/stroke_project/data/healthcare-dataset-stroke-data.csv\"\n",
        "\n",
        "#Reduce output logging, making easier to read\n",
        "logger = logging.getLogger(\"py4j\")\n",
        "logger.setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Helper Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Cleaning functions\n",
        "# Define null tokens\n",
        "null_token_list = [t.lower() for t in [\"na\", \"n/a\", \"null\", \"none\", \"n.a.\", \"na.\", \"\", \"N/A\"]]\n",
        "\n",
        "#Function to convert null tokens to actual nulls\n",
        "def convert_to_null(column_name: str):\n",
        "    \"\"\"\n",
        "    Convert common text-based null tokens to actual null for the given column.\n",
        "    \"\"\"\n",
        "    return F.when(F.lower(F.trim(F.col(column_name))).isin(null_token_list), None).otherwise(F.col(column_name))\n",
        "\n",
        "\n",
        "#Function to clean column names\n",
        "def clean_column_names(dataframe: DataFrame):\n",
        "    \"\"\"\n",
        "    Standardize column names: trim, lowercase, replace spaces/hyphens with underscores, remove parentheses.\n",
        "    \"\"\"\n",
        "    for original_name in dataframe.columns:\n",
        "        standardized_name = (\n",
        "            original_name.strip()\n",
        "                         .lower()\n",
        "                         .replace(\" \", \"_\")\n",
        "                         .replace(\"-\", \"_\")\n",
        "                         .replace(\"(\", \"\")\n",
        "                         .replace(\")\", \"\")\n",
        "        )\n",
        "        if standardized_name != original_name:\n",
        "            dataframe = dataframe.withColumnRenamed(original_name, standardized_name)\n",
        "    return dataframe\n",
        "\n",
        "\n",
        "#Function to normalize numeric columns \n",
        "def normalize_numeric(dataframe: DataFrame):\n",
        "    return (\n",
        "        dataframe.withColumn(\"bmi\", convert_to_null(\"bmi\").cast(\"double\"))\n",
        "                 .withColumn(\"avg_glucose_level\", F.col(\"avg_glucose_level\").cast(\"double\"))\n",
        "                 .withColumn(\"age\", F.col(\"age\").cast(\"double\"))\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Handling class imbalance with oversampling\n",
        "def replicate_minority_class_with_oversampling(dataframe: DataFrame, target_column: str = label_column):\n",
        "    \"\"\"\n",
        "    Oversample the minority class in a binary classification DataFrame by replication.\n",
        "    \"\"\"\n",
        "    # Count records per class label\n",
        "    class_count_by_label = {\n",
        "        int(row[target_column]): int(row[\"count\"])\n",
        "        for row in dataframe.groupBy(target_column).count().collect()\n",
        "    }\n",
        "\n",
        "    # Identify minority and majority class labels\n",
        "    minority_label_value = 1\n",
        "    majority_label_value = 0\n",
        "    minority_count = class_count_by_label.get(minority_label_value, 1)\n",
        "    majority_count = class_count_by_label.get(majority_label_value, 1)\n",
        "\n",
        "    # If classes are already balanced return original DataFrame\n",
        "    if minority_count >= majority_count:\n",
        "        return dataframe\n",
        "\n",
        "    # Separate minority and majority class DataFrames\n",
        "    minority_dataframe = dataframe.filter(F.col(target_column) == minority_label_value)\n",
        "    majority_dataframe = dataframe.filter(F.col(target_column) == majority_label_value)\n",
        "\n",
        "    # Calculate replication factor\n",
        "    replication_factor = max(1, majority_count // max(1, minority_count))\n",
        "\n",
        "    # Create balanced DataFrame by replicating minority class\n",
        "    balanced_dataframe = majority_dataframe\n",
        "    for _ in range(replication_factor):\n",
        "        balanced_dataframe = balanced_dataframe.union(minority_dataframe)\n",
        "\n",
        "    #Print oversampling details\n",
        "    print(\n",
        "        f\"Oversampled minority class from {minority_count} \"\n",
        "        f\"to approximately {minority_count * (replication_factor + 1)}; \"\n",
        "        f\"majority was {majority_count}.\"\n",
        "    )\n",
        "\n",
        "    # Shuffle the resulting balanced DataFrame\n",
        "    return balanced_dataframe.randomSplit([1.0], seed=42)[0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocessing():\n",
        "    \"\"\"\n",
        "    Preprocessing:\n",
        "      - Impute numeric columns (median)\n",
        "      - Index + OneHotEncode categorical string columns\n",
        "      - Assemble all features into a single vector\n",
        "      - Standardize features\n",
        "    \"\"\"\n",
        "    #Define columns\n",
        "    numeric_feature_columns = [\"age\", \"avg_glucose_level\", \"bmi\"]\n",
        "    categorical_string_columns = [\"gender\", \"ever_married\", \"work_type\", \"residence_type\", \"smoking_status\"]\n",
        "    binary_numeric_columns = [\"hypertension\", \"heart_disease\"]\n",
        "\n",
        "    #Imputer for numeric columns\n",
        "    numeric_imputer = Imputer(\n",
        "        inputCols=numeric_feature_columns,\n",
        "        outputCols=numeric_feature_columns\n",
        "    )\n",
        "\n",
        "    #StringIndexers and OneHotEncoders for categorical columns\n",
        "    string_indexers = [\n",
        "        StringIndexer(inputCol=col_name, outputCol=f\"{col_name}_index\", handleInvalid=\"keep\")\n",
        "        for col_name in categorical_string_columns\n",
        "    ]\n",
        "\n",
        "    one_hot_encoders = [\n",
        "        OneHotEncoder(inputCol=f\"{col_name}_index\", outputCol=f\"{col_name}_onehot\")\n",
        "        for col_name in categorical_string_columns\n",
        "    ]\n",
        "\n",
        "    #Create single feature vector\n",
        "    assembled_inputs = (\n",
        "        numeric_feature_columns\n",
        "        + binary_numeric_columns\n",
        "        + [f\"{col_name}_onehot\" for col_name in categorical_string_columns]\n",
        "    )\n",
        "\n",
        "    #Combine all features into a single vector\n",
        "    feature_assembler = VectorAssembler(\n",
        "        inputCols=assembled_inputs,\n",
        "        outputCol=\"unscaled_features\"\n",
        "    )\n",
        "\n",
        "    #Standardize features\n",
        "    feature_scaler = StandardScaler(\n",
        "        inputCol=\"unscaled_features\",\n",
        "        outputCol=\"features\",\n",
        "        withMean=False,\n",
        "        withStd=True\n",
        "    )\n",
        "    \n",
        "    #Return preprocessing stages\n",
        "    return [numeric_imputer] + string_indexers + one_hot_encoders + [feature_assembler, feature_scaler]\n",
        "\n",
        "\n",
        "#Function to figure out feature vector length\n",
        "def infer_feature_vector_length(fitted_preprocessing_pipeline, dataframe: DataFrame, features_column: str = \"features\"):\n",
        "    sample_row = fitted_preprocessing_pipeline.transform(dataframe.limit(1)).select(features_column).collect()[0]\n",
        "    return len(sample_row[features_column])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluator Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Function to evaluate model performance\n",
        "def evaluate_models(predictions: DataFrame, label_column_name: str = label_column):\n",
        "    \"\"\"\n",
        "    Compute metrics:\n",
        "      - accuracy, AUC (ROC), precision, recall (sensitivity), specificity\n",
        "      - confusion matrix: TP, TN, FP, FN\n",
        "    Returns a dictionary.\n",
        "    \"\"\"\n",
        "    # Find AUC\n",
        "    auc_evaluator = BinaryClassificationEvaluator(labelCol=label_column_name, metricName=\"areaUnderROC\")\n",
        "    area_under_roc = auc_evaluator.evaluate(predictions)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    total_records = predictions.count()\n",
        "    correct_predictions = predictions.filter(F.col(\"prediction\") == F.col(label_column_name)).count()\n",
        "    accuracy = (correct_predictions / total_records) if total_records else 0.0\n",
        "\n",
        "    # Confusion matrix \n",
        "    true_positive = predictions.filter((F.col(label_column_name) == 1) & (F.col(\"prediction\") == 1)).count()\n",
        "    false_positive = predictions.filter((F.col(label_column_name) == 0) & (F.col(\"prediction\") == 1)).count()\n",
        "    false_negative = predictions.filter((F.col(label_column_name) == 1) & (F.col(\"prediction\") == 0)).count()\n",
        "    true_negative = predictions.filter((F.col(label_column_name) == 0) & (F.col(\"prediction\") == 0)).count()\n",
        "\n",
        "    # Calculate precision, recall, specificity\n",
        "    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) else 0.0\n",
        "    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) else 0.0\n",
        "    specificity = true_negative / (true_negative + false_positive) if (true_negative + false_positive) else 0.0\n",
        "\n",
        "    # Return metrics as a dictionary\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"auc\": area_under_roc,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"specificity\": specificity,\n",
        "        \"true_positive\": true_positive,\n",
        "        \"true_negative\": true_negative,\n",
        "        \"false_positive\": false_positive,\n",
        "        \"false_negative\": false_negative,\n",
        "        \"total_records\": total_records,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Training \n",
        "def models():\n",
        "    spark_session = SparkSession.builder.appName(\"sat5165_stroke_pipeline\").getOrCreate()\n",
        "    spark_session.sparkContext.setLogLevel(\"ERROR\")\n",
        "    print(f\"Loading: {csv_input_path}\")\n",
        "    \n",
        "    input_dataframe = spark_session.read.csv(csv_input_path, header=True, inferSchema=True)\n",
        "\n",
        "    input_dataframe = clean_column_names(input_dataframe)\n",
        "    input_dataframe = normalize_numeric(input_dataframe).dropna(subset=[label_column]).withColumn(label_column, F.col(label_column).cast(\"int\"))\n",
        "\n",
        "    # Train/test split\n",
        "    training_raw_dataframe, testing_dataframe = input_dataframe.randomSplit([0.8, 0.2], seed=42)\n",
        "    training_dataframe = replicate_minority_class_with_oversampling(training_raw_dataframe, label_column)\n",
        "\n",
        "    # Preprocessing pipeline\n",
        "    preprocessing_stages = preprocessing()\n",
        "\n",
        "    # Fit once to get MLP input size\n",
        "    fitted_preprocessor = Pipeline(stages=preprocessing_stages).fit(training_dataframe)\n",
        "    input_dim = infer_feature_vector_length(fitted_preprocessor, training_dataframe)\n",
        "    print(f\"[info] MLP input_dim = {input_dim}\")\n",
        "\n",
        "    # List to hold (name, estimator, grid)\n",
        "    model_specs = []\n",
        "\n",
        "    # Logistic Regression\n",
        "    logistic_regression = LogisticRegression(labelCol=label_column, featuresCol=\"features\")\n",
        "    lr_grid = ParamGridBuilder().addGrid(logistic_regression.regParam, [0.01, 0.1]).build()\n",
        "    model_specs.append((\"Logistic Regression\", logistic_regression, lr_grid))\n",
        "\n",
        "    # Linear SVM\n",
        "    linear_svm = LinearSVC(labelCol=label_column, featuresCol=\"features\", maxIter=100, regParam=0.1)\n",
        "    svm_grid = ParamGridBuilder().addGrid(linear_svm.regParam, [0.01, 0.1, 0.5]).build()\n",
        "    model_specs.append((\"Linear SVM\", linear_svm, svm_grid))\n",
        "\n",
        "    # Gradient-Boosted Trees\n",
        "    gbt = GBTClassifier(labelCol=label_column, featuresCol=\"features\", maxIter=50, maxDepth=5, stepSize=0.1, seed=42)\n",
        "    gbt_grid = ParamGridBuilder().addGrid(gbt.maxDepth, [3, 5]).addGrid(gbt.stepSize, [0.05, 0.1]).build()\n",
        "    model_specs.append((\"Gradient Boosted Trees\", gbt, gbt_grid))\n",
        "\n",
        "    # Multilayer Perceptron (MLP)\n",
        "    mlp = MultilayerPerceptronClassifier(\n",
        "        labelCol=label_column,\n",
        "        featuresCol=\"features\",\n",
        "        layers=[input_dim, 64, 32, 2],\n",
        "        maxIter=100,\n",
        "        stepSize=0.05,\n",
        "        blockSize=128,\n",
        "        seed=42\n",
        "    )\n",
        "    mlp_grid = ParamGridBuilder().addGrid(mlp.maxIter, [100]).build()\n",
        "    model_specs.append((\"Multilayer Perceptron\", mlp, mlp_grid))\n",
        "\n",
        "    # Evaluator (AUC-ROC)\n",
        "    auc_evaluator = BinaryClassificationEvaluator(labelCol=label_column, metricName=\"areaUnderROC\")\n",
        "\n",
        "    summary = []\n",
        "    for name, estimator, param_grid in model_specs:\n",
        "        pipeline_with_estimator = Pipeline(stages=preprocessing_stages + [estimator])\n",
        "        cross_validator = CrossValidator(\n",
        "            estimator=pipeline_with_estimator,\n",
        "            estimatorParamMaps=param_grid,\n",
        "            evaluator=auc_evaluator,\n",
        "            numFolds=3,\n",
        "            parallelism=2,\n",
        "            seed=42\n",
        "        )\n",
        "        print(f\"\\nTraining {name}...\")\n",
        "        start_time = time.time()\n",
        "        best_model = cross_validator.fit(training_dataframe)\n",
        "        train_seconds = time.time() - start_time\n",
        "        print(f\"Total time: {train_seconds:.2f} seconds\")\n",
        "\n",
        "        predictions = best_model.transform(testing_dataframe)\n",
        "        metrics = evaluate_models(predictions, label_column)\n",
        "        metrics[\"model\"] = name\n",
        "        metrics[\"train_seconds\"] = train_seconds\n",
        "        summary.append(metrics)\n",
        "\n",
        "        print(f\"\\n{name} Performance Results\")\n",
        "        for k in [\"accuracy\", \"auc\", \"precision\", \"recall\", \"specificity\"]:\n",
        "            print(f\"  {k}: {metrics[k]:.3f}\")\n",
        "        print(\n",
        "            f\"  TP: {metrics['true_positive']}  TN: {metrics['true_negative']}  \"\n",
        "            f\"FP: {metrics['false_positive']}  FN: {metrics['false_negative']}\"\n",
        "        )\n",
        "\n",
        "    # Summary table\n",
        "    print(\"MODEL COMPARISON SUMMARY\")\n",
        "    print(\"-\" * 86)\n",
        "    print(f\"{'Model':<28} {'Acc':>8} {'AUC':>8} {'Prec':>8} {'Recall':>8} {'Spec':>8} {'Time(s)':>9}\")\n",
        "    print(\"-\" * 86)\n",
        "    for r in summary:\n",
        "        print(f\"{r['model']:<28} {r['accuracy']:>8.3f} {r['auc']:>8.3f} {r['precision']:>8.3f} {r['recall']:>8.3f} {r['specificity']:>8.3f} {r['train__seconds']:>9.2f}\")\n",
        "\n",
        "    spark_session.stop()\n",
        "    print(\"[Stroke Pipeline Complete.]\")\n",
        "\n",
        "    spark_session.stop()\n",
        "    print(\"[Stroke Pipeline Complete.]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Run the program\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"[main] starting models()\")\n",
        "    models()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
